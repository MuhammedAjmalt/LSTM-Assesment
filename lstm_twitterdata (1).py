# -*- coding: utf-8 -*-
"""LSTM_Twitterdata.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1znsDrnwuePNBDv4tKS0E75GvrPevnimu
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import pandas as pd
import string
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import matplotlib.pyplot as plt
# %matplotlib inline

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing
import os
import tweepy as tw #for accessing Twitter API


#For Preprocessing
import re    # RegEx for removing non-letter characters
import nltk  #natural language processing
nltk.download("stopwords")
from nltk.corpus import stopwords
from nltk.stem.porter import *

# For Building the model
from sklearn.model_selection import train_test_split
import tensorflow as tf
import seaborn as sns

#For data visualization
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
# %matplotlib inline

pd.options.plotting.backend = "plotly"

data =pd.read_csv(r"/content/drive/MyDrive/dataset/train (2).csv")

data

data.tail(5)

test_data =pd.read_csv(r"/content/drive/MyDrive/dataset/test (1).csv")

test_data

data.shape

data.describe()

data.Tweets

df_new=data[['Tweets','label']]

df_new.label.value_counts()

df_new.isnull().sum()

df_new['Tweets'][25]

df_0=data[data['label']=='Negative']
df_1=data[data['label']=='Positive']

print(df_1)

from wordcloud import WordCloud
plt.figure(figsize=(20,20))
wc=WordCloud(max_words=500,width=800, height=300).generate(" ".join(df_0.Tweets))
plt.imshow(wc, interpolation='bilinear')
plt.title("word cloud for target 0", fontsize=14)

from wordcloud import WordCloud
plt.figure(figsize=(20,20))
wc=WordCloud(max_words=500,width=800, height=300).generate(" ".join(df_1.Tweets))
plt.imshow(wc, interpolation='bilinear')
plt.title("word cloud for target 1", fontsize=14)

#all text are enclosed in the quotes so we are removing it and alotting a space instead
for i, col in enumerate(df_new.columns):
  df_new.iloc[:, i]=df_new.iloc[:, i].astype(str).str.replace("'", '')

df_new.head()

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('wordnet')

#creating the list of stopwords
stop =stopwords.words('english')

# initializing lemmatization
lemmatizer=WordNetLemmatizer()

from bs4 import BeautifulSoup
import re

def text_to_words(raw_text):
      Tweets = BeautifulSoup(raw_text,'html.parser').get_text() #removing any HTML tags
      letters_only=re.sub('[^a-zA-z]',' ',Tweets) # removing any non-aplhabetic characters with space
      words=letters_only.lower().split()   #tokenizing
      no_stopwords=[w for w in words if not w in stop]   #removing stop words
      lemmatize_words=[lemmatizer.lemmatize(w) for w in no_stopwords]   # lemmatization
      return (" ".join(lemmatize_words))

df_new['text_clean']=data['Tweets'].apply(text_to_words)

df_new.head()

X=df_new.text_clean
y=df_new.label

X_train,X_test, y_train,y_test=train_test_split(X, y, stratify=y,test_size=0.2,random_state=0)

count_vectorizer=CountVectorizer(stop_words='english')   # bag of words
x_train_bow=count_vectorizer.fit_transform(X_train)
x_test_bow=count_vectorizer.transform(X_test)

x_train_bow

from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from sklearn.metrics import classification_report

# Define LSTM model
model = Sequential()
model.add(Embedding(input_dim=x_train_bow.shape[1], output_dim=100, input_length=x_train_bow.shape[1]))
model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(units=1, activation='sigmoid'))

# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Print model summary
model.summary()

# Train model
history = model.fit(x_train_bow, y_train, epochs=5, batch_size=64, validation_data=(x_test_bow, y_test))

# Evaluate model
score = model.evaluate(x_test_bow, y_test)
print("Test Accuracy:", score[1])

# Get predictions
y_pred = model.predict_classes(x_test_bow)

# Print classification report
print(classification_report(y_test, y_pred))



